{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c8de1-0fb8-4012-8d3f-47766069bdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notes:\n",
    "    - The number of cores for parallelization can be adjusted by the \"cuda_cores\" parameter.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import multiprocessing as mp\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\", force=True)   #  fork  \n",
    "except RuntimeError:\n",
    "    pass\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from joblib import Parallel, delayed\n",
    "PARALLEL_KW = dict(\n",
    "    backend=\"loky\",\n",
    "    temp_folder=\"./tmp/joblib\",\n",
    "    max_nbytes=None,\n",
    "    mmap_mode=None,\n",
    "    # inner_max_num_threads=1,   #   joblib>=1.3\n",
    ")\n",
    "os.makedirs(\"./tmp/joblib\", exist_ok=True)\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)  #  \n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import itertools\n",
    "\n",
    "\"\"\" the proposed functions \"\"\"\n",
    "import Cind_gaussian_fun\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\"\"\"Initialize the random number generator with a fixed seed.\"\"\"\n",
    "def seed_torch(seed=42):\n",
    "    \"\"\"For reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\"\"\" Select GPU if available; otherwise use CPU and cap BLAS/Torch threads to 1 to avoid oversubscription. \"\"\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"1\"  \n",
    "    os.environ[\"MKL_NUM_THREADS\"] = \"1\"   \n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "    os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "    os.environ[\"OMP_WAIT_POLICY\"] = \"PASSIVE\"\n",
    "    os.environ[\"KMP_INIT_AT_FORK\"] = \"FALSE\"\n",
    "    \n",
    "    os.environ[\"TORCH_NUM_THREADS\"] = \"1\"            \n",
    "    os.environ[\"TORCH_NUM_INTEROP_THREADS\"] = \"1\"   \n",
    "\n",
    "print(device)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Define the log function\n",
    "\"\"\"\n",
    "def init_logging(log_file):\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "     \n",
    "    file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "     \n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    if not logger.handlers:\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0db37-7c74-44c1-b1e6-9854cf3769bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21., 63., 30., 23., 63., 58., 73., 73., 23., 29., 29.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 55/55 [00:00<00:00, 63.07it/s]\n"
     ]
    }
   ],
   "source": [
    "#-------------------- before COVID-19 period: 1 January 2016 - 31 December 2018 ----------------------------\n",
    "#### process data\n",
    "name1 = [\"CS\" , \"CD\", \"CSt\" , \"Eng\"  ,\"Fin\" ,\"HC\" , \"Ind\" , \"IT\" , \"Mat\" , \"RE\" , \"Uti\"]\n",
    "combi_name1 = list(itertools.combinations(name1, 2))\n",
    "\n",
    "shape = torch.zeros(11)\n",
    "\n",
    "# read data\n",
    "title_z = '68return_' + name1[0] + '.csv'\n",
    "data = torch.from_numpy(pd.read_csv(title_z).iloc[:, 1:].to_numpy())\n",
    "shape[0] = data.shape[1]\n",
    "for t in range(10):\n",
    "        title_zz = '68return_' + name1[t+1] + '.csv'\n",
    "        zz = torch.from_numpy(pd.read_csv(title_zz).iloc[:, 1:].to_numpy())\n",
    "        shape[t+1] = zz.shape[1]\n",
    "        data = torch.cat((data, zz), dim=1)\n",
    "\n",
    "combinations = list(itertools.combinations(range(11), 2))\n",
    "print(shape)\n",
    "\n",
    "# read data\n",
    "stacked_tensor = torch.zeros(55, data.shape[0], data.shape[1])\n",
    "for i in range(55):\n",
    "    mask = torch.ones(data.shape[1], dtype=torch.bool)\n",
    "    index = combinations[i]\n",
    "    # print(index)\n",
    "    indx1 = int( torch.sum(shape[:index[0]]) )\n",
    "    indx2 = int( torch.sum(shape[:index[0]+1]) )\n",
    "    indy1 = int( torch.sum(shape[:index[1]]) )\n",
    "    indy2 = int( torch.sum(shape[:index[1]+1]) )\n",
    "    x = data[:, int(indx1):int(indx2)] \n",
    "    #print(x.shape)\n",
    "    y = data[:, int(indy1):int(indy2)] \n",
    "    # print(y.shape)\n",
    "    mask[indx1:indx2] = False   \n",
    "    mask[indy1:indy2] = False   \n",
    "    z = data[:, mask]   \n",
    "    # print(z.shape)\n",
    "    dat = torch.cat((x, y, z), dim=1)\n",
    "    stacked_tensor[i] = dat\n",
    "    # print(i)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "alpha=0.05\n",
    "\n",
    "\n",
    "# define the function to implement the proposed conditional independence test (CI-FNN) with sample splitting and selected n_3^{opt}\n",
    "def run_simulation(i, stacked_tensor, combinations, alpha):\n",
    "    #dat = stacked_tensor[i,:,:]\n",
    "    x = data[:, 0: int(shape[combinations[i][0]])] \n",
    "    y = data[:, int(shape[combinations[i][0]]): int(shape[combinations[i][0]] + shape[combinations[i][1]])] \n",
    "    z = data[:, int(shape[combinations[i][0]] + shape[combinations[i][1]]): data.shape[1]] \n",
    "    seed_torch((i + 1) * 12345)\n",
    "\n",
    "    res_orig = Cind_gaussian_fun.Cind_Gtest_py(device, \"S-selectn3\", 'all', x, y, z, alpha, batchsize=32, \n",
    "                                              hidden_features1 = 128, hidden_features2 = 32,  lr=0.01,n_epochs= 400, patience=30,drop_last1=False)\n",
    "    \n",
    "    order = ['Gaussian', 'Mammen', 'Rademacher']\n",
    "    result =  [float(res_orig[k]['p_value']) for k in order]\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "cuda_cores = 55     # the number of CPU cores\n",
    "n_my = 55\n",
    "sim_list = {'my_data_ex8': 0}\n",
    "\n",
    "order = ['Gaussian', 'Mammen', 'Rademacher']  # 3 columns\n",
    "\n",
    "# conduct experiments using parallel processing\n",
    "for sim, dep in sim_list.items():\n",
    "    try:\n",
    "        with Parallel(n_jobs=cuda_cores) as parallel:\n",
    "            tmp_results = parallel(\n",
    "                delayed(run_simulation)(i, stacked_tensor, combinations, alpha)\n",
    "                for i in tqdm(range(n_my))\n",
    "            )\n",
    "        # Convert all results into a single NumPy array\n",
    "        M = np.asarray(tmp_results, dtype=float)          # n_my-by-3 matrix\n",
    "        results_tensor = torch.from_numpy(M).float().cpu()   \n",
    "\n",
    "        # Build DataFrame\n",
    "        df = pd.DataFrame(M, columns=order)\n",
    "\n",
    "        first_names  = [a for a, b in combi_name1]\n",
    "        second_names = [b for a, b in combi_name1]\n",
    "        df.insert(0, 'Name1', first_names)\n",
    "        df.insert(1, 'Name2', second_names)\n",
    "\n",
    "        dic_path = os.path.join('.', 'realdata')    # create a folder\n",
    "        os.makedirs(dic_path, exist_ok=True)\n",
    "\n",
    "        # save to a csv file\n",
    "        file_path = os.path.join(dic_path, '68pval_nn.csv')\n",
    "        df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "\n",
    "        print(results_tensor)  # n_my-by-3 matrix\n",
    "\n",
    "        # Cleanup\n",
    "        del tmp_results, M, df, results_tensor\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    except Exception:\n",
    "        error_message = f\"code is wrong:\\n{traceback.format_exc()}\"\n",
    "        print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f109f99-a4d1-47a6-bf52-eb278ba990ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------- during/after COVID-19 period: 1 January 2020 - 31 December 2022 ---------------------------------------------\n",
    "#### process data\n",
    "name1 = [\"CS\" , \"CD\", \"CSt\" , \"Eng\"  ,\"Fin\" ,\"HC\" , \"Ind\" , \"IT\" , \"Mat\" , \"RE\" , \"Uti\"]\n",
    "combi_name1 = list(itertools.combinations(name1, 2))  \n",
    "\n",
    "shape = torch.zeros(11)\n",
    "# read data\n",
    "title_z = '02return_' + name1[0] + '.csv'\n",
    "data = torch.from_numpy(pd.read_csv(title_z).iloc[:, 1:].to_numpy())\n",
    "shape[0] = data.shape[1]\n",
    "for t in range(10):\n",
    "        title_zz = '02return_' + name1[t+1] + '.csv'\n",
    "        zz = torch.from_numpy(pd.read_csv(title_zz).iloc[:, 1:].to_numpy())\n",
    "        shape[t+1] = zz.shape[1]\n",
    "        data = torch.cat((data, zz), dim=1)\n",
    "\n",
    "combinations = list(itertools.combinations(range(11), 2))\n",
    "print(shape)\n",
    "\n",
    "# read data\n",
    "stacked_tensor = torch.zeros(55, data.shape[0], data.shape[1])\n",
    "for i in range(55):\n",
    "    mask = torch.ones(data.shape[1], dtype=torch.bool)\n",
    "    index = combinations[i]\n",
    "    # print(index)\n",
    "    indx1 = int( torch.sum(shape[:index[0]]) )\n",
    "    indx2 = int( torch.sum(shape[:index[0]+1]) )\n",
    "    indy1 = int( torch.sum(shape[:index[1]]) )\n",
    "    indy2 = int( torch.sum(shape[:index[1]+1]) )\n",
    "    x = data[:, int(indx1):int(indx2)] \n",
    "    #print(x.shape)\n",
    "    y = data[:, int(indy1):int(indy2)] \n",
    "    # print(y.shape)\n",
    "    mask[indx1:indx2] = False   \n",
    "    mask[indy1:indy2] = False   \n",
    "    z = data[:, mask]   \n",
    "    # print(z.shape)\n",
    "    dat = torch.cat((x, y, z), dim=1)\n",
    "    stacked_tensor[i] = dat\n",
    "    # print(i)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "alpha=0.05\n",
    "\n",
    "\n",
    "# define the function to implement the proposed conditional independence test (CI-FNN) with sample splitting and selected n_3^{opt}\n",
    "def run_simulation(i, stacked_tensor, combinations, alpha):\n",
    "    #dat = stacked_tensor[i,:,:]\n",
    "    x = data[:, 0: int(shape[combinations[i][0]])] \n",
    "    y = data[:, int(shape[combinations[i][0]]): int(shape[combinations[i][0]] + shape[combinations[i][1]])] \n",
    "    z = data[:, int(shape[combinations[i][0]] + shape[combinations[i][1]]): data.shape[1]] \n",
    "    seed_torch((i + 1) * 12345)\n",
    "\n",
    "    res_orig = Cind_gaussian_fun.Cind_Gtest_py(device, \"S-selectn3\", 'all', x, y, z, alpha, batchsize=32, \n",
    "                                              hidden_features1 = 128, hidden_features2 = 32,  lr=0.01,n_epochs= 400, patience=30,drop_last1=False)\n",
    "    \n",
    "    order = ['Gaussian', 'Mammen', 'Rademacher']\n",
    "    result =  [float(res_orig[k]['p_value']) for k in order]\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "cuda_cores = 55    # the number of CPU cores\n",
    "n_my = 55\n",
    "sim_list = {'my_data_ex8': 0}\n",
    "\n",
    "order = ['Gaussian', 'Mammen', 'Rademacher']  # 3 columns\n",
    "\n",
    "# conduct experiments using parallel processing\n",
    "for sim, dep in sim_list.items():\n",
    "    try:\n",
    "        with Parallel(n_jobs=cuda_cores) as parallel:\n",
    "            tmp_results = parallel(\n",
    "                delayed(run_simulation)(i, stacked_tensor, combinations, alpha)\n",
    "                for i in tqdm(range(n_my))\n",
    "            )\n",
    "        # Convert all results into a single NumPy array\n",
    "        M = np.asarray(tmp_results, dtype=float)          # n_my-by-3 matrix\n",
    "        results_tensor = torch.from_numpy(M).float().cpu()   \n",
    "\n",
    "        # Build DataFrame\n",
    "        df = pd.DataFrame(M, columns=order)\n",
    "\n",
    "        first_names  = [a for a, b in combi_name1]\n",
    "        second_names = [b for a, b in combi_name1]\n",
    "        df.insert(0, 'Name1', first_names)\n",
    "        df.insert(1, 'Name2', second_names)\n",
    "\n",
    "        dic_path = os.path.join('.', 'realdata')   # create a folder\n",
    "        os.makedirs(dic_path, exist_ok=True)\n",
    "        # save to a csv file\n",
    "        file_path = os.path.join(dic_path, '02pval_nn.csv')\n",
    "        df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "\n",
    "        print(results_tensor)  # n_my-by-3 matrix\n",
    "\n",
    "        # Cleanup\n",
    "        del tmp_results, M, df, results_tensor\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    except Exception:\n",
    "        error_message = f\"code is wrong:\\n{traceback.format_exc()}\"\n",
    "        print(error_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
